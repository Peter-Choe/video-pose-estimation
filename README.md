
# ğŸ¯ Video Pose Estimation

2D Skeleton Keypointsë¥¼ ì˜ìƒì—ì„œ ì¶”ì¶œí•˜ëŠ” ë¹„ë™ê¸° ê¸°ë°˜ API íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.  
FastAPI, Celery, Redis, Triton Inference Server ê¸°ë°˜ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ONNX ë³€í™˜ ë° ì‹¤ì‹œê°„ ì¶”ë¡  ìµœì í™”ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

---

## ğŸ§© ì£¼ìš” êµ¬ì„± ìš”ì†Œ

| êµ¬ì„± ìš”ì†Œ             | ì„¤ëª… |
|----------------------|------|
| **FastAPI**          | ì˜ìƒ ì—…ë¡œë“œ ë° ì¶”ë¡  ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ” REST API |
| **Celery + Redis**   | ë¹„ë™ê¸° ì˜ìƒ ì²˜ë¦¬ ì‘ì—…ì„ íë¡œ ë¶„ì‚° ì‹¤í–‰ |
| **NVIDIA Triton Server**    | ONNX ë³€í™˜ëœ MMPose ëª¨ë¸ì„ ê³ ì„±ëŠ¥ìœ¼ë¡œ ì¶”ë¡  ì„œë¹™ |
| **ONNX Export ì»¨í…Œì´ë„ˆ** | MMPose í•™ìŠµ ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜ |
| **Docker Compose**   | ì „ êµ¬ì„± ìš”ì†Œë¥¼ ì»¨í…Œì´ë„ˆ ê¸°ë°˜ìœ¼ë¡œ í†µí•© ì‹¤í–‰ |

---

## âš™ï¸ ì‹œìŠ¤í…œ êµ¬ì¡° ë° ì‹¤í–‰

```
Client â†’ FastAPI â†’ Celery â†’ Triton â†’ ê²°ê³¼ ì €ì¥ ë° ì‹œê°í™”
```

- ì „ì²´ êµ¬ì„± ì»¨í…Œì´ë„ˆí™”: `docker compose up --build`
- ë¡œì»¬ ë‹¨ë… ì‹¤í–‰ë„ ì§€ì›: FastAPI, Celery ë³„ë„ ì‹¤í–‰ ê°€ëŠ¥

---

## ğŸ“Š ì¶”ë¡  ì„±ëŠ¥ ë¹„êµ ìš”ì•½

> í…ŒìŠ¤íŠ¸ í™˜ê²½: WSL2 + Ubuntu 22.04 + Triton Inference Server  
> GPU: **NVIDIA GeForce RTX 3060 Laptop (6GB VRAM)**  
> âš ï¸ ë…¸íŠ¸ë¶ GPU ì„±ëŠ¥ì€ ë°ìŠ¤í¬íƒ‘ ëŒ€ë¹„ ì•½ 30% ë‚®ì„ ìˆ˜ ìˆìŒ

| ì¡°ê±´                      | í‰ê·  ì²˜ë¦¬ ì‹œê°„ | GPU ì‚¬ìš©ë¥  | ë¹„ê³  |
|---------------------------|----------------|-------------|------|
| Non-batch (1 ìš”ì²­)        | ì•½ 85.5ì´ˆ      | ~50%        | ê¸°ì¤€ì„  |
| Non-batch (3 concurrent)  | ì•½ 127.5ì´ˆ     | ~55%        | ì„±ëŠ¥ ì €í•˜, í ë³‘ëª© |
| Batch-16 (4 concurrent)   | ì•½ 86.5ì´ˆ      | ~80%        | ì„±ëŠ¥ íšŒë³µ, GPU íš¨ìœ¨ â†‘ |
| Batch-32 (4 concurrent)   | ì•½ 91.1ì´ˆ      | ~83%        | ì•½ê°„ ëŠë¦¼, ëŒ€ê¸° ì˜¤ë²„í—¤ë“œ ê°€ëŠ¥ |

---

## ğŸ§ª Batching ê¸°ë°˜ ë³‘ë ¬ ì¶”ë¡  ì‹¤í—˜

- **Batch Size** ì¦ê°€ ì‹œ GPU í™œìš©ë¥ ì´ í–¥ìƒë˜ë©° ì²˜ë¦¬ ì„±ëŠ¥ì´ ê°œì„ ë¨
- **Celery + Triton** êµ¬ì¡°ì—ì„œ ë‹¤ìˆ˜ ì˜ìƒ ë™ì‹œ ì²˜ë¦¬ ì‹œ íš¨ìœ¨ì„± ì¦ê°€
- **GPU ì‚¬ìš©ë¥ **: Batching ì ìš© ì‹œ ìµœëŒ€ 87%ê¹Œì§€ ë„ë‹¬

---

## ğŸ” ì‚¬ìš© ëª¨ë¸ êµ¬ì¡°: MMPose ê¸°ë°˜ MobileNetV2

| êµ¬ì„±       | ì„¤ëª…                                         |
|------------|----------------------------------------------|
| Backbone   | MobileNetV2 â€“ ê²½ëŸ‰í™”ëœ CNN êµ¬ì¡°               |
| Head       | HeatmapHead â€“ 17ê°œ ê´€ì ˆì— ëŒ€í•´ Gaussian Heatmap ì˜ˆì¸¡ |
| ì…ë ¥ í¬ê¸°  | 192Ã—256                                      |
| ì¶œë ¥ í¬ê¸°  | 48Ã—64 Ã— 17 keypoints                          |

> Gaussian Heatmapì€ ê° keypoint ì¢Œí‘œë¥¼ 2D ì •ê·œë¶„í¬ë¡œ í‘œí˜„í•˜ì—¬ í•™ìŠµ ì•ˆì •ì„±ê³¼ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.  
> COCO formatì˜ 17ê°œ ê´€ì ˆì„ ì˜ˆì¸¡í•˜ë©°, ONNX ë³€í™˜ í›„ Tritonì—ì„œ ì„œë¹™ë©ë‹ˆë‹¤.

---

## ğŸ”„ YOLO ì—°ë™ í™•ì¥ì„±

- í˜„ì¬ëŠ” **ì¤‘ì•™ì— ìœ„ì¹˜í•œ ë‹¨ì¼ ì¸ë¬¼**ì„ cropí•˜ì—¬ í¬ì¦ˆë¥¼ ì¶”ë¡ í•©ë‹ˆë‹¤.
- ì‚¬ìš©í•œ MMPose ëª¨ë¸ì€ **ì‚¬ëŒ ê°ì§€ ê¸°ëŠ¥ ì—†ì´**, cropëœ ì¸ë¬¼ ì´ë¯¸ì§€ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.
- ì¶”í›„ YOLO ë“±ê³¼ ì—°ë™í•˜ë©´ **ì—¬ëŸ¬ ì‚¬ëŒì— ëŒ€í•œ bboxë¥¼ ìë™ ì¶”ì¶œ**í•˜ì—¬, ê° ì¸ë¬¼ì˜ í¬ì¦ˆë¥¼ ê°œë³„ ì¶”ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì´ëŠ” **ì‹¤ì‹œê°„ í–‰ë™ ì¸ì‹**ì´ë‚˜ **AI ê´‘ê³  ë¶„ì„** ì‹œìŠ¤í…œ ë“±ìœ¼ë¡œì˜ í™•ì¥ì— í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡° ìš”ì•½

```
video-pose-estimation-api/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ fastapi_pipeline/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ tasks.py
â”‚   â”‚   â”œâ”€â”€ infer_client.py
â”‚   â”‚   â””â”€â”€ core/
â”œâ”€â”€ onnx_export/
â”‚   â”œâ”€â”€ export_mmpose_to_onnx.py
â”‚   â””â”€â”€ models/
â”œâ”€â”€ models/                           # Triton ì„œë¹™ìš©
â”œâ”€â”€ resources/video/                 # ì…ë ¥ ì˜ìƒ
â”œâ”€â”€ resources/output_video/          # ê²°ê³¼ ì˜ìƒ
â”œâ”€â”€ logs/                            # ì¶”ë¡  ë¡œê·¸
â”œâ”€â”€ test_api.py
â””â”€â”€ run_local_inference_visual.py
```

---

## ğŸ§ª ë¡œì»¬ ê°œë°œ í™˜ê²½ ì‹¤í–‰ ë°©ë²•

### 1. Triton + Redis ì»¨í…Œì´ë„ˆ ì‹¤í–‰
```bash
docker compose -f docker-compose.local.yml up
```
> `localhost:8000` (Triton), `localhost:6379` (Redis) ë…¸ì¶œë¨

### 2. FastAPI ì„œë²„ ì‹¤í–‰
```bash
uvicorn fastapi_pipeline.app.main:app --reload --port 5000
```

### 3. Celery ì›Œì»¤ ì‹¤í–‰
```bash
celery -A fastapi_pipeline.app.core.celery_app worker --loglevel=info
```
> `.env.local` ë˜ëŠ” `.env.docker`ë¥¼ í†µí•œ í™˜ê²½ë³€ìˆ˜ ì„¤ì • í•„ìš”

---

## ğŸš€ ì „ì²´ ì»¨í…Œì´ë„ˆë¡œ ì‹¤í–‰ (ë°°í¬ ë˜ëŠ” í†µí•© í…ŒìŠ¤íŠ¸ìš©)

```bash
docker compose up --build
```
> FastAPI + Celery + Redis + Triton Inference Serverê°€ í•œ ë²ˆì— ì‹¤í–‰ë©ë‹ˆë‹¤.

---

## ğŸ”„ MMPose â†’ ONNX ë³€í™˜ ë°©ë²•

### 1. ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€ ë¹Œë“œ
```bash
cd onnx_export
docker build -t mmpose2onnx .
```

### 2. ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
```bash
docker run --rm -v $(pwd):/workspace mmpose2onnx python export_mmpose_to_onnx.py
```

- ë³€í™˜ëœ ëª¨ë¸: `onnx_export/models/mobilenetv2_pose/model.onnx`
- Tritonì—ì„œ ì„œë¹™ ì‹œ, `models/mobilenetv2_pose/1/model.onnx` ìœ„ì¹˜ì— ë°°ì¹˜ í•„ìš”

---

## ğŸ¥ ì˜ìƒ ë°ëª¨

ì¶”ë¡  ê²°ê³¼ ì˜ˆì‹œëŠ” ì•„ë˜ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- â–¶ï¸ [output_pose_Djokovic_compressed.mp4](resources/output_video/output_pose_Djokovic_compressed.mp4)
- â–¶ï¸ [output_pose.mp4](resources/output_video/output_pose.mp4)

---

## ğŸ“¡ API ì‚¬ìš© ì˜ˆì‹œ

### âœ… 1. ì „ì²´ ì‹œìŠ¤í…œ ì‹¤í–‰ í›„ í…ŒìŠ¤íŠ¸
```bash
python test_api.py
```

- `resources/video/`ì˜ mp4 ì˜ìƒ ì—…ë¡œë“œ
- Celery task ë“±ë¡ ë° task_id ë°˜í™˜
- ê²°ê³¼ ë¡œê·¸ëŠ” `logs/`ì— ì €ì¥ ë˜ëŠ” ì½˜ì†” ì¶œë ¥ í™•ì¸ ê°€ëŠ¥

| ë¡œê·¸ íŒŒì¼                          | ì„¤ëª… |
|----------------------------------|------|
| `logs/video_pose_inference.log` | ì¶”ë¡  ë‹¨ê³„ë³„ ë¡œê·¸ |
| `logs/video_inference_times.log`| ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡ |

---

### âœ… 2. ë¡œì»¬ Triton ì¶”ë¡  í…ŒìŠ¤íŠ¸ (GUI ì‹œê°í™”)

```bash
ENV=local python run_local_inference_visual.py
```

- Triton ì„œë²„ì— ì§ì ‘ ì¶”ë¡  ìš”ì²­
- ì‹œê°í™”ëœ keypointë¥¼ ì‹¤ì‹œê°„ ì¶œë ¥ (`DISPLAY` í™˜ê²½ í•„ìš”)
- ê²°ê³¼ëŠ” `resources/output_video/`ì— mp4 íŒŒì¼ë¡œ ì €ì¥

## âš ï¸ ëª¨ë¸ í•™ìŠµ ë° ì„±ëŠ¥ ê´€ë ¨ ì°¸ê³ 

- ë³¸ í”„ë¡œì íŠ¸ëŠ” MMPoseì—ì„œ ì œê³µí•˜ëŠ” **ì‚¬ì „ í•™ìŠµ ëª¨ë¸(weight)**ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•˜ì˜€ìŠµë‹ˆë‹¤.
- **Fine-tuning**ì´ë‚˜ **ì •ëŸ‰ì  í‰ê°€**ëŠ” ìˆ˜í–‰í•˜ì§€ ì•Šì•˜ìœ¼ë©°, ì…ë ¥ í’ˆì§ˆì— ë”°ë¼ ì„±ëŠ¥ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- í–¥í›„ ê°œì„  ë°©í–¥ìœ¼ë¡œëŠ” ì‚¬ìš©ì ë°ì´í„° ê¸°ë°˜ í•™ìŠµ, keypoint ê¸°ë°˜ ë™ì‘ ë¶„ë¥˜ ëª¨ë¸ ì—°ë™, OKS ë“± í‰ê°€ ì§€í‘œ ì ìš© ë“±ì´ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ§° ê¸°ìˆ  ìŠ¤íƒ ìš”ì•½

- **Backend**: Python, FastAPI, Celery
- **Inference**: Triton Inference Server, ONNX, Torch
- **Pose Model**: MMPose (Top-down, MobileNetV2)
- **Infra**: Docker, Redis, Docker Compose
- **í…ŒìŠ¤íŠ¸ í™˜ê²½**: WSL2 + **RTX 3060 Laptop (6GB VRAM)**

---
